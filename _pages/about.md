---
permalink: /
title: "About me"
excerpt: "About me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

Hi! This is Minxing Zhang (张敏行), and I'm a second-year PhD student in <a href="https://cispa.de/en" target="_blank">CISPA</a> supervised by Prof. <a href="https://michaelbackes.eu/" target="_blank">Michael Backes</a> and Dr. <a href="https://xiao-zhang.net/" target="_blank">Xiao Zhang</a>. I obtained my B.S. degree in Computer Science and Technology from <a href="https://www.sdu.edu.cn/" target="_blank">Shandong University</a> (2020) advised by Dr. <a href="https://renzhaochun.github.io/" target="_blank">Zhaochun Ren</a>.

My CV is available [here](CV.pdf).

---

_Happy that I have worked with many intelligent researchers._
_Looking forward to future collaborations on interesting projects._

---

### Research Interests

- Trustworthy Machine Learning
- Security and Privacy
- Model Behavior Explanation

---

### What's New!

- [10/2024] Our paper titled <a href="https://arxiv.org/abs/2310.04539" target="_blank">Generating Less Certain Adversarial Examples Improves Robust Generalization</a> is accepted by <a href="https://openreview.net/forum?id=MMtK0kUML7" target="_blank">TMLR</a>.
- [10/2024] Our paper titled <a href="https://arxiv.org/abs/2410.07670" target="_blank">Invisibility Cloak: Disappearance under Human Pose Estimation via Backdoor Attacks</a> is available online, which proposed the first disappearance attack against HPE via backdoor techniques.
- [08/2024] My <a href="https://github.com/minxingzhang/PGD" target="_blank">implementations of PGD-based adversarial training</a> are available online.
- [08/2024] Our paper titled <a href="https://arxiv.org/abs/2408.00129" target="_blank">Vera Verto: Multimodal Hijacking Attack</a> is available online, which hijacks a CV victim model to implement the adversary's own NLP task with stealthiness.
- [10/2023] Our paper titled <a href="https://ieeexplore.ieee.org/document/10484149" target="_blank">Generated Distributions Are All You Need for Membership Inference Attacks Against Generative Models</a> is accepted by <a href="https://wacv2024.thecvf.com/" target="_blank">IEEE/CVF WACV2024</a>.
- [10/2023] Our paper titled <a href="https://arxiv.org/abs/2310.04539" target="_blank">Generating Less Certain Adversarial Examples Improves Robust Generalization</a> is available online, which proposes to improve robust generalization by our novel definition _Adversarial Certainty_.
- [10/2022] Start my PhD study at CISPA.
- [09/2021] Our paper titled <a href="https://dl.acm.org/doi/10.1145/3460120.3484770" target="_blank">Membership Inference Attacks Against Recommender Systems</a> is accepted by <a href="https://www.sigsac.org/ccs/CCS2021/" target="_blank">ACM CCS 2021</a>.
- [05/2021] Join CISPA as a preparatory-phase student.
- [06/2020] Obtain my bachelor's degree from Shandong University.
